{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network and Q-learning to beat you at tic tac toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, what do we need\n",
    "#A way of encoding states. A 3x3 matrix will do\n",
    "#0 will mean no tick there. 1 is player 1, 2 is player 2\n",
    "#So we need a function to evaluate the board and return a) reward of taking an action,\n",
    "#And which actions are allowable\n",
    "def valid_actions(state):\n",
    "    if state.shape!=(3,3):\n",
    "        raise Exception('States are supposed to be size 3x3')\n",
    "    return np.where(state==0)\n",
    "def choose_action(actions,Q,epsilon,state):\n",
    "    if(actions[0].shape==(0,)):\n",
    "        raise Exception('Attempted action while no legal moves allowed')\n",
    "    idx=np.random.permutation(list(range(len(actions[0]))))[:1]\n",
    "    best_action=Q[stringify(state)]\n",
    "    return (actions[0][idx],actions[1][idx])\n",
    "def act(action,player,state):\n",
    "    if(type(action)!=tuple or len(action)!=2):\n",
    "        raise Exception('Actions are supposed to be 2xtuple')\n",
    "    state2=state.copy()\n",
    "    state2[action[0],action[1]]=player\n",
    "    return state2\n",
    "def check_win(state):\n",
    "    #When have you won at tictactoe? When there are 3 of player in a row,column\n",
    "    #Or diagonal\n",
    "    if((state[::-1].diagonal()==1).all() or\n",
    "       (state[::-1].diagonal()==1).all() or\n",
    "       ((state==1).sum(0)==3).any() or\n",
    "       ((state==1).sum(1)==3).any()):\n",
    "        return 1\n",
    "    #Player 2 wins\n",
    "    elif((state[::-1].diagonal()==2).all() or\n",
    "       (state[::-1].diagonal()==2).all() or\n",
    "       ((state==2).sum(0)==2).any() or\n",
    "       ((state==2).sum(1)==2).any()):\n",
    "        return 2\n",
    "    #Nothing happens\n",
    "    else:\n",
    "        return 3\n",
    "def check_end(state):\n",
    "    if(valid_actions(state)[0].shape==(0,)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def blank_state():\n",
    "    return np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "def stringify(array):\n",
    "    if(type(array)==np.ndarray):\n",
    "        return ''.join(str(array.flatten()))\n",
    "    elif(type(array)==tuple):\n",
    "        return ''.join(str(np.array([np.asscalar(array[0]),np.asscalar(array[1])]).flatten()))\n",
    "        \n",
    "def max_action(Q,state,action):\n",
    "    return np.max([i for i in Q[stringify(state)].values()])\n",
    "\n",
    "def R(state,action,player):\n",
    "    w=check_win(state)\n",
    "    if(w==player):\n",
    "        return 1000\n",
    "    elif(w!=player and w!=3):\n",
    "        return -1000\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "ALPHA=1\n",
    "GAMMA=0.9\n",
    "Q=defaultdict(dict)\n",
    "Q2=defaultdict(dict)\n",
    "def learn(n=100):   \n",
    "    for i in range(n):\n",
    "        state=blank_state()\n",
    "        for j in range(99):\n",
    "            #Player 1 moves\n",
    "            epsilon=0.9*np.exp(-j)\n",
    "            action=choose_action(valid_actions(state),Q,epsilon,state)\n",
    "            #print('Player one moves',action)\n",
    "            state2=act(action,1,state)\n",
    "            #print('State now is\\n',state)\n",
    "            try:\n",
    "                Q[stringify(state)][stringify(action)]\n",
    "            except:            \n",
    "                Q[stringify(state)][stringify(action)]=0        \n",
    "            Q[stringify(state)][stringify(action)]+=ALPHA*(R(state,action,1)\n",
    "            +GAMMA*max_action(Q,state,action)\n",
    "            -Q[stringify(state)][stringify(action)])\n",
    "            state=state2\n",
    "            if(check_end(state)): break\n",
    "            #Player 2 moves\n",
    "            action=choose_action(valid_actions(state),Q2,epsilon,state)\n",
    "            state2=act(action,2,state)\n",
    "            try:\n",
    "                Q2[stringify(state)][stringify(action)]\n",
    "            except:           \n",
    "                Q2[stringify(state)][stringify(action)]=0\n",
    "            Q2[stringify(state)][stringify(action)]=+Q2[stringify(state)][stringify(action)]\n",
    "            +ALPHA*(R(state,action,2)+GAMMA*max_action(Q2,state,action)\n",
    "            -Q2[stringify(state)][stringify(action)])\n",
    "            state=state2\n",
    "            if(check_end(state)): break\n",
    "    \n",
    "              \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I run the profiler and it seems that this takes too much time because of the stringify operations. States should be stored as numbers. I can either precompute all states and store them and assign a number to them or store them as a dictionary containing dictionaries of actions as numbers linked to rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f learn learn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{111: [1, 2]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={111:[1,2]}\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
