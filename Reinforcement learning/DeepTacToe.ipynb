{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network and Q-learning to beat you at tic tac toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "def nested_dict():\n",
    "    return collections.defaultdict(nested_dict)\n",
    "ALPHA=1\n",
    "GAMMA=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input np.array([0,0,0,0,0,0,0,0]),1\n",
    "class Qmatrix():\n",
    "    def __init__(self):\n",
    "        self.data=defaultdict(nested_dict)\n",
    "    def set(self,state,action,value):        \n",
    "        self.data[(state,action)]=value\n",
    "    def get(self,state,action=None):\n",
    "        return self.data[(state,action)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, what do we need\n",
    "#A way of encoding states. A 3x3 matrix will do\n",
    "#0 will mean no tick there. 1 is player 1, 2 is player 2\n",
    "#So we need a function to evaluate the board and return a) reward of taking an action,\n",
    "#And which actions are allowable\n",
    "#State will be a tuple, action will be int.\n",
    "def valid_actions(state):\n",
    "    if state.shape!=(9,):\n",
    "        raise Exception('States are supposed to be size 9x1')\n",
    "    return np.where(state==0)[0]\n",
    "def choose_action(actions,Q,epsilon,state):\n",
    "    if(actions[0].shape==(0,)):\n",
    "        raise Exception('Attempted action while no legal moves allowed')\n",
    "    idx=np.random.permutation(list(range(len(actions))))[:1]\n",
    "    best_action=Q[state]\n",
    "    return (actions[idx],actions[idx])\n",
    "def act(action,player,state):\n",
    "    if(type(action)!=array):\n",
    "        raise Exception('Actions are supposed to be arrays')\n",
    "    state2=state.copy()\n",
    "    state2[action]=player\n",
    "    return state2\n",
    "def available_states(state,player):\n",
    "    b=np.argwhere((state==0)).flatten()\n",
    "    states=np.tile(a,len(b)).reshape(len(b),9)\n",
    "    for i,pos in enumerate(b):\n",
    "        states[i][pos]=player\n",
    "def check_win(state):\n",
    "    #When have you won at tictactoe? When there are 3 of player in a row,column\n",
    "    #Or diagonal    \n",
    "    if((state.reshape(3,3)[::-1].diagonal()==1).all() or\n",
    "       (state[::-1].reshape(3,3).diagonal()==1).all() or\n",
    "       ((state.reshape(3,3)==1).sum(0)==3).any() or\n",
    "       ((state.reshape(3,3)==1).sum(1)==3).any()):\n",
    "        return 1\n",
    "    #Player 2 wins\n",
    "    elif((state[::-1].reshape(3,3).diagonal()==2).all() or\n",
    "       (state.reshape(3,3)[::-1].diagonal()==2).all() or\n",
    "       ((state.reshape(3,3)==2).sum(0)==2).any() or\n",
    "       ((state.reshape(3,3)==2).sum(1)==2).any()):\n",
    "        return 2\n",
    "    #Nothing happens\n",
    "    else:\n",
    "        return 3\n",
    "def check_end(state):\n",
    "    if(valid_actions(state).shape==(0,)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def blank_state():\n",
    "    return np.array([0,0,0,0,0,0,0,0,0])\n",
    "     \n",
    "def max_action(Q,state,action):\n",
    "    return np.max([i for i in Q[state].values()])\n",
    "\n",
    "def R(state,action,player):\n",
    "    w=check_win(state)\n",
    "    if(w==player):\n",
    "        return 1000\n",
    "    elif(w!=player and w!=3):\n",
    "        return -1000\n",
    "    else:\n",
    "        return 0\n",
    "def do_turn(Q,epsilon,state,player,learning=0):\n",
    "    action=choose_action(valid_actions(state),Q,epsilon,state)\n",
    "    state2=act(action,player,state)\n",
    "    if learning:\n",
    "        try:\n",
    "            Q.get(state,action)\n",
    "        except:           \n",
    "            Q.set(state,action,0)\n",
    "        Q.set(state,action,Q.get(state,action)\n",
    "        +ALPHA*(R(state,action,player)+GAMMA*max_action(Q,state,action)\n",
    "        -Q.get(state,action)))\n",
    "    state=state2\n",
    "\n",
    "\n",
    "Q=Qmatrix()\n",
    "def learn(n=100):   \n",
    "    for i in range(n):\n",
    "        state=blank_state()\n",
    "        for j in range(99):\n",
    "            #Player 1 moves\n",
    "            epsilon=0.9*np.exp(-j)\n",
    "            do_turn(Q,epsilon,state,1,learning=1)\n",
    "            if(check_end(state)): break\n",
    "            do_turn(Q,epsilon,state,2,learning=0)\n",
    "            if(check_end(state)): break           \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I run the profiler and it seems that this takes too much time because of the stringify operations. States should be stored as numbers. I can either precompute all states and store them and assign a number to them or store them as a dictionary containing dictionaries of actions as numbers linked to rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer arrays with one element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-e8b3f422fa0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-e0a280702c52>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m#Player 1 moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mdo_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdo_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-e0a280702c52>\u001b[0m in \u001b[0;36mdo_turn\u001b[0;34m(Q, epsilon, state, player, learning)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mstate2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-e0a280702c52>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(actions, Q, epsilon, state)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted action while no legal moves allowed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbest_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer arrays with one element can be converted to an index"
     ]
    }
   ],
   "source": [
    "learn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887104172047142"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
