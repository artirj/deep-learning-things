{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 860M (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 5103)\n",
      "/home/artir/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = 'data/glove/'\n",
    "res_path = path+'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "This section shows how we processed the original glove text files. However, there's no need for you to do this, since we provide the [pre-processed glove data](www.platform.ai/models/glove)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_glove(name):\n",
    "    with open(path+ 'glove.' + name + '.txt', 'r') as f: lines = [line.split() for line in f]\n",
    "    words = [d[0] for d in lines]\n",
    "    vecs = np.stack(np.array(d[1:], dtype=np.float32) for d in lines)\n",
    "    wordidx = {o:i for i,o in enumerate(words)}\n",
    "    save_array(res_path+name+'.dat', vecs)\n",
    "    pickle.dump(words, open(res_path+name+'_words.pkl','wb'))\n",
    "    pickle.dump(wordidx, open(res_path+name+'_idx.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_glove('6B.50d')\n",
    "get_glove('6B.100d')\n",
    "get_glove('6B.200d')\n",
    "get_glove('6B.300d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Looking at the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After you've downloaded the [pre-processed glove data](www.platform.ai/models/glove), you should use `tar -zxf` to untar them, and put them in the path that {res_path} points to. (If you don't have a great internet connection, feel free to only download the 50d version, since that's what we'll be using in class). \n",
    "\n",
    "Then the following function will return the word vectors as a matrix, the word list, and the mapping from word to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_glove(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/glove/results/6B.50d.dat/meta/sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3d9d0357284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'6B.50d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-76c1d3f9444c>\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m(loc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     return (load_array(loc+'.dat'),\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_words.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         pickle.load(open(loc+'_idx.pkl','rb')))\n",
      "\u001b[0;32m/home/artir/Escritorio/Machine Learning/AWS/utils.py\u001b[0m in \u001b[0;36mload_array\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbcolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/artir/anaconda3/lib/python3.5/site-packages/bcolz/toplevel.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(rootdir, mode)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbcolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbcolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mbcolz/carray_ext.pyx\u001b[0m in \u001b[0;36mbcolz.carray_ext.carray.__cinit__ (bcolz/carray_ext.c:13429)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mbcolz/carray_ext.pyx\u001b[0m in \u001b[0;36mbcolz.carray_ext.carray._read_meta (bcolz/carray_ext.c:17967)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/glove/results/6B.50d.dat/meta/sizes'"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_glove(res_path+'6B.50d')\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's the first 25 \"words\" in glove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "' '.join(words[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is how you can look up a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def w2v(w): return vecs[wordidx[w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v('of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just for fun, let's take a look at a 2d projection of the first 350 words, using [T-SNE](http://distill.pub/2016/misread-tsne/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "Y = tsne.fit_transform(vecs[:500])\n",
    "\n",
    "start=0; end=350\n",
    "dat = Y[start:end]\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(dat[:, 0], dat[:, 1])\n",
    "for label, x, y in zip(words[start:end], dat[:, 0], dat[:, 1]):\n",
    "    plt.text(x,y,label, color=np.random.rand(3)*0.7,\n",
    "                 fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
