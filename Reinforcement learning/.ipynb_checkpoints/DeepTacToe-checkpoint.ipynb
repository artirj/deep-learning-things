{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network and Q-learning to beat you at tic tac toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "ALPHA=1\n",
    "GAMMA=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input np.array([0,0,0,0,0,0,0,0]),1\n",
    "class Tree():\n",
    "    def __init__(self):\n",
    "        self.data=defaultdict(nested_dict)\n",
    "    def set(self,key,key2,value):        \n",
    "        self.data[key[0]][key[1]][key[2]][key[3]][key[4]][key[5]][key[6]][key[7]][key[8]][key2]=value\n",
    "    def get(self,key,key2=''):\n",
    "        if(key2==''):\n",
    "             return self.data[key[0]][key[1]][key[2]][key[3]][key[4]][key[5]][key[6]][key[7]][key[8]]\n",
    "        return self.data[key[0]][key[1]][key[2]][key[3]][key[4]][key[5]][key[6]][key[7]][key[8]][key2]\n",
    "    def get_max_child(self,state):\n",
    "        return np.max(list(a.get(state).values()))\n",
    "a=Tree()\n",
    "a.set([0,0,0,0,0,0,0,0,0],7,1)\n",
    "a.set([0,0,0,0,0,0,0,0,0],6,2)\n",
    "a.get_max_child([0,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(list(a.get([0,0,0,0,0,0,0,0,0]).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, what do we need\n",
    "#A way of encoding states. A 3x3 matrix will do\n",
    "#0 will mean no tick there. 1 is player 1, 2 is player 2\n",
    "#So we need a function to evaluate the board and return a) reward of taking an action,\n",
    "#And which actions are allowable\n",
    "def valid_actions(state):\n",
    "    if state.shape!=(9,):\n",
    "        raise Exception('States are supposed to be size 9x1')\n",
    "    return np.where(state==0)[0]\n",
    "def choose_action(actions,Q,epsilon,state):\n",
    "    if(actions[0].shape==(0,)):\n",
    "        raise Exception('Attempted action while no legal moves allowed')\n",
    "    idx=np.random.permutation(list(range(len(actions))))[:1]\n",
    "    best_action=Q[state]\n",
    "    return (actions[idx],actions[idx])\n",
    "def act(action,player,state):\n",
    "    if(type(action)!=array):\n",
    "        raise Exception('Actions are supposed to be arrays')\n",
    "    state2=state.copy()\n",
    "    state2[action]=player\n",
    "    return state2\n",
    "def check_win(state):\n",
    "    #When have you won at tictactoe? When there are 3 of player in a row,column\n",
    "    #Or diagonal\n",
    "    \n",
    "    if((state.reshape(3,3)[::-1].diagonal()==1).all() or\n",
    "       (state[::-1].reshape(3,3).diagonal()==1).all() or\n",
    "       ((state.reshape(3,3)==1).sum(0)==3).any() or\n",
    "       ((state.reshape(3,3)==1).sum(1)==3).any()):\n",
    "        return 1\n",
    "    #Player 2 wins\n",
    "    elif((state[::-1].reshape(3,3).diagonal()==2).all() or\n",
    "       (state.reshape(3,3)[::-1].diagonal()==2).all() or\n",
    "       ((state.reshape(3,3)==2).sum(0)==2).any() or\n",
    "       ((state.reshape(3,3)==2).sum(1)==2).any()):\n",
    "        return 2\n",
    "    #Nothing happens\n",
    "    else:\n",
    "        return 3\n",
    "def check_end(state):\n",
    "    if(valid_actions(state).shape==(0,)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def blank_state():\n",
    "    return np.array([0,0,0,0,0,0,0,0,0])\n",
    "     \n",
    "def max_action(Q,state,action):\n",
    "    return np.max([i for i in Q[state].values()])\n",
    "\n",
    "def R(state,action,player):\n",
    "    w=check_win(state)\n",
    "    if(w==player):\n",
    "        return 1000\n",
    "    elif(w!=player and w!=3):\n",
    "        return -1000\n",
    "    else:\n",
    "        return 0\n",
    "def do_turn(Q,epsilon,state,player):\n",
    "    action=choose_action(valid_actions(state),Q,epsilon,state)\n",
    "    state2=act(action,player,state)\n",
    "    try:\n",
    "        Q[state][action]\n",
    "    except:           \n",
    "        Q[state][action]=0\n",
    "    Q[state][action]=+Q[state][action]\n",
    "    +ALPHA*(R(state,action,player)+GAMMA*max_action(Q,state,action)\n",
    "    -Q[state][action])\n",
    "    state=state2\n",
    "\n",
    "\n",
    "Q=Tree()\n",
    "Q2=Tree()\n",
    "def learn(n=100):   \n",
    "    for i in range(n):\n",
    "        state=blank_state()\n",
    "        for j in range(99):\n",
    "            #Player 1 moves\n",
    "            epsilon=0.9*np.exp(-j)\n",
    "            do_turn(Q,epsilon,state,1)\n",
    "            if(check_end(state)): break\n",
    "            do_turn(Q2,epsilon,state,2)\n",
    "            if(check_end(state)): break\n",
    "    \n",
    "              \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I run the profiler and it seems that this takes too much time because of the stringify operations. States should be stored as numbers. I can either precompute all states and store them and assign a number to them or store them as a dictionary containing dictionaries of actions as numbers linked to rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f learn learn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(None, {})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
